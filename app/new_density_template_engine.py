from __future__ import annotations

"""
New Density Template Engine for Issue #246

DEPRECATED: This module is marked as deprecated and scheduled for replacement in Phase 2.
Please use `density_template_engine.py` as the primary entry point.

Implements the new report structure as specified in Issue #246:
1. Title & Metadata
2. Executive Summary
3. Methodology & Inputs
4. Start Times & Cohorts
5. Course Overview
6. Flagged Segments â€” Complete List
7. Flagged Bins Summary
8. Operational Heatmap (placeholder)
9. Segment Details
10. Mitigations
11. Appendix
"""

import warnings
warnings.warn(
    "This module is marked as deprecated and scheduled for replacement in Phase 2. "
    "Please use `density_template_engine.py` as the primary entry point.",
    DeprecationWarning
)
import os
from typing import Dict, List, Any, Optional
from datetime import datetime
import pandas as pd


class NewDensityTemplateEngine:
    """Template engine for the new density report structure per Issue #246."""
    
    def __init__(self):
        self.template_sections = [
            'title_metadata',
            'executive_summary', 
            'methodology_inputs',
            'start_times_cohorts',
            'course_overview',
            'flagged_segments_complete',
            'flagged_bins_summary',
            'operational_heatmap_placeholder',
            'segment_details',
            'mitigations',
            'appendix'
        ]
    
    def generate_report(
        self,
        context: Dict[str, Any],
        bins_df: pd.DataFrame,
        segments_df: pd.DataFrame,
        segment_windows_df: pd.DataFrame,
        flagged_bins_df: pd.DataFrame,
        segment_summary_df: pd.DataFrame,
        stats: Dict[str, Any]
    ) -> str:
        """Generate the complete new density report."""
        
        content = []
        
        # 1. Title & Metadata
        content.append(self._generate_title_metadata(context))
        content.append("")
        content.append("---")
        content.append("")
        
        # 2. Executive Summary
        content.append(self._generate_executive_summary(stats, segment_summary_df))
        content.append("")
        content.append("---")
        content.append("")
        
        # 3. Methodology & Inputs
        content.append(self._generate_methodology_inputs(context))
        content.append("")
        content.append("---")
        content.append("")
        
        # 4. Start Times & Cohorts
        content.append(self._generate_start_times_cohorts(context))
        content.append("")
        content.append("---")
        content.append("")
        
        # 5. Course Overview
        content.append(self._generate_course_overview(segments_df, segment_windows_df, bins_df))
        content.append("")
        content.append("---")
        content.append("")
        
        # 6. Flagged Segments
        content.append(self._generate_flagged_segments_complete(segment_summary_df, segments_df))
        content.append("")
        content.append("---")
        content.append("")
        
        # 7. Flagged Bins Summary (merged into Flagged Segments)
        # content.append(self._generate_flagged_bins_summary(segment_summary_df))
        # content.append("")
        # content.append("---")
        # content.append("")
        
        # 8. Operational Heatmap (placeholder)
        content.append(self._generate_operational_heatmap_placeholder())
        content.append("")
        content.append("---")
        content.append("")
        
        # 9. Bin-Level Detail
        content.append(self._generate_bin_level_detail(flagged_bins_df))
        content.append("")
        content.append("---")
        content.append("")
        
        # 10. Segment Details
        content.append(self._generate_segment_details(segments_df, segment_windows_df, segment_summary_df))
        content.append("")
        content.append("---")
        content.append("")
        
        # 10. Mitigations
        content.append(self._generate_mitigations(segment_summary_df))
        content.append("")
        content.append("---")
        content.append("")
        
        # 11. Appendix
        content.append(self._generate_appendix())
        
        return "\n".join(content)
    
    def _generate_title_metadata(self, context: Dict[str, Any]) -> str:
        """Generate title and metadata section."""
        lines = [
            "# Fredericton Marathon â€” Density Report",
            f"**Schema:** {context.get('schema_version', '1.0.0')}",
            f"**Method:** segments_from_bins",
            f"**Date:** {context.get('report_date', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))}",
            f"**Inputs:** bins.parquet, segments.parquet, segment_windows_from_bins.parquet",
            f"**App:** v{context.get('app_version', '1.0.0')}"
        ]
        return "\n".join(lines)
    
    def _generate_executive_summary(self, stats: Dict[str, Any], segment_summary_df: pd.DataFrame) -> str:
        """Generate executive summary with operational status."""
        lines = [
            "## Executive Summary"
        ]
        
        # Key metrics
        # Convert peak_rate_per_m_per_min to p/s for display
        peak_rate_per_m_per_min = stats.get('peak_rate_per_m_per_min', 0)
        peak_rate_ps = peak_rate_per_m_per_min / 60.0  # Convert to persons/second
        
        lines.extend([
            f"- **Peak Density:** {stats.get('peak_density', 0):.4f} p/mÂ² (LOS {stats.get('worst_los', 'A')})",
            f"- **Peak Rate:** {peak_rate_ps:.2f} p/s",
            f"- **Segments with Flags:** {len(segment_summary_df[segment_summary_df['flagged_bins'] > 0])} / {len(segment_summary_df)}",
            f"- **Flagged Bins:** {stats.get('flagged_bins', 0)} / {stats.get('total_bins', 0)}"
        ])
        
        # Operational Status
        worst_severity = stats.get('worst_severity', 'none')
        if worst_severity == 'critical':
            operational_status = "ðŸ”´ Action Required"
            alert_reason = "Critical density/rate conditions detected"
        elif worst_severity == 'watch':
            operational_status = "âš ï¸ Attention Required"
            alert_reason = "Watch conditions detected"
        else:
            operational_status = "âœ… All Clear"
            alert_reason = "No operational flags"
        
        lines.extend([
            f"- **Operational Status:** {operational_status} ({alert_reason})",
            "",
            "> LOS (Level of Service) describes how comfortable runners are within a section â€” A means free-flowing, while E/F indicate crowding. Even when overall LOS is good, short-lived surges in runner flow can stress aid stations or intersections, requiring active flow management."
        ])
        
        return "\n".join(lines)
    
    def _generate_methodology_inputs(self, context: Dict[str, Any]) -> str:
        """Generate methodology and inputs section."""
        lines = [
            "## Methodology & Inputs",
            f"- **Window Size:** {context.get('window_s', 30)} s; **Bin Size:** {context.get('bin_km', 0.2)} km",
            "",
            "### LOS and Rate Triggers (from Rulebook)",
            "- **LOS thresholds** define crowding levels based on density (p/mÂ²):",
            "  - A: < 0.36 | B: 0.36â€“0.54 | C: 0.54â€“0.72 | D: 0.72â€“1.08 | E: 1.08â€“1.63 | F: > 1.63",
            "- **Rate thresholds** define throughput risk based on flow references (persons/m/min):",
            f"  - Warning: {context.get('rate_warn_threshold', 0):.1f} | Critical: {context.get('rate_critical_threshold', 0):.1f}",
            "",
            "These thresholds come from the Fredericton Marathon rulebook and align with crowd management standards for mass participation events."
        ]
        return "\n".join(lines)
    
    def _generate_start_times_cohorts(self, context: Dict[str, Any]) -> str:
        """Generate start times and cohorts section."""
        # Get runner counts from context if available
        full_count = context.get('full_runners', 368)
        tenk_count = context.get('10k_runners', 618)
        half_count = context.get('half_runners', 912)
        
        lines = [
            "## Start Times & Cohorts",
            f"- **Full Marathon** â€” 07:00 ({full_count:,} runners)",
            f"- **10K** â€” 07:20 ({tenk_count:,} runners)",
            f"- **Half Marathon** â€” 07:40 ({half_count:,} runners)",
            "",
            "> Bins may include runners from multiple events as waves overlap in time."
        ]
        return "\n".join(lines)
    
    def _generate_course_overview(self, segments_df: pd.DataFrame, segment_windows_df: pd.DataFrame, bins_df: pd.DataFrame) -> str:
        """Generate course overview table."""
        lines = [
            "## Course Overview",
            "",
            "| Segment | Label | Schema | Width (m) | Spatial Bins |",
            "|----------|--------|--------|-----------|--------------|"
        ]
        
        # Calculate spatial bin counts per segment (unique start_km values)
        spatial_bins = bins_df.groupby('segment_id')['start_km'].nunique().to_dict()
        
        # Sort segments by segment_id for course order
        segments_sorted = segments_df.sort_values('segment_id').copy()
        
        # Add spatial bin counts
        segments_sorted['spatial_bins'] = segments_sorted['segment_id'].map(spatial_bins).fillna(0).astype(int)
        
        for _, row in segments_sorted.iterrows():
            lines.append(
                f"| {row['segment_id']} | {row['seg_label']} | {row['segment_type']} | "
                f"{row['width_m']} | {row['spatial_bins']} |"
            )
        
        lines.append("")
        lines.append("> Note: Each spatial bin is analyzed across 80 time windows (30-second intervals). Total space-time bins per segment = spatial bins Ã— 80 (e.g., A1: 5 Ã— 80 = 400; I1: 121 Ã— 80 = 9,680).")
        
        return "\n".join(lines)
    
    def _generate_flagged_segments_complete(self, segment_summary_df: pd.DataFrame, segments_df: pd.DataFrame) -> str:
        """Generate complete list of flagged segments (no truncation)."""
        lines = [
            "## Flagged Segments",
            "",
            "| Segment | Label | Flagged Bins | Total Bins | % | Worst Bin (km) | Time | Density (p/mÂ²) | Rate (p/s) | Util% | LOS | Severity | Reason |",
            "|----------|--------|--------------|------------|---|----------------|-------|----------------|-------------|-------|-----|-----------|---------|"
        ]
        
        # Merge with segments to get segment_type for Util% calculation
        flagged_with_schema = segment_summary_df[segment_summary_df['flagged_bins'] > 0].merge(
            segments_df[['segment_id', 'segment_type']], on='segment_id', how='left'
        ).sort_values('segment_id')
        
        if len(flagged_with_schema) == 0:
            lines.append("| *No flagged segments* | | | | | | | | | | | | |")
        else:
            for _, row in flagged_with_schema.iterrows():
                # Format worst bin km range
                worst_km = f"{row['worst_bin_start_km']:.1f}-{row['worst_bin_end_km']:.1f}"
                
                # Format worst bin time (HH:MM only)
                if pd.notna(row['worst_bin_t_start']):
                    t_start = row['worst_bin_t_start']
                    # Handle pandas Timestamp
                    if hasattr(t_start, 'strftime'):
                        worst_time = t_start.strftime('%H:%M')
                    # Handle string timestamps
                    elif isinstance(t_start, str):
                        # Extract HH:MM from ISO format (e.g., "2025-10-16T07:20:00Z")
                        if 'T' in t_start:
                            time_part = t_start.split('T')[1]
                            worst_time = time_part[:5]  # HH:MM
                        else:
                            worst_time = t_start[:5]
                    else:
                        worst_time = str(t_start)[:5]
                else:
                    worst_time = "N/A"
                
                # Format worst bin rate
                worst_rate = f"{row['worst_bin_rate']:.3f}" if row['worst_bin_rate'] > 0 else "N/A"
                
                # Calculate Util% based on segment schema
                segment_type = row['segment_type']
                flow_ref_critical = None
                if segment_type == 'start_corral':
                    flow_ref_critical = 600
                elif segment_type == 'on_course_narrow':
                    flow_ref_critical = 400
                
                util_display = "N/A"
                if flow_ref_critical and row['peak_rate_per_m_per_min'] > 0:
                    util_display = f"{(row['peak_rate_per_m_per_min'] / flow_ref_critical * 100):.0f}%"
                
                lines.append(
                    f"| {row['segment_id']} | {row['seg_label']} | {row['flagged_bins']} | {row['total_bins']} | "
                    f"{row['flagged_percentage']:.1f}% | {worst_km} | {worst_time} | {row['worst_bin_density']:.4f} | {worst_rate} | {util_display} | "
                    f"{row['worst_bin_los']} | {row['worst_severity']} | {row['worst_reason']} |"
                )
        
        return "\n".join(lines)
    
    def _generate_flagged_bins_summary(self, segment_summary_df: pd.DataFrame) -> str:
        """Generate flagged bins summary by segment."""
        lines = [
            "## Flagged Bins Summary",
            "",
            "| Segment | Label | Flagged | Total | % | Top Reasons |",
            "|----------|--------|----------|--------|----|--------------|"
        ]
        
        for _, row in segment_summary_df.iterrows():
            if row['flagged_bins'] > 0:
                lines.append(
                    f"| {row['segment_id']} | {row['seg_label']} | {row['flagged_bins']} | "
                    f"{row['total_bins']} | {row['flagged_percentage']:.1f}% | {row['worst_reason']} |"
                )
        
        if len(segment_summary_df[segment_summary_df['flagged_bins'] > 0]) == 0:
            lines.append("| *No flagged segments* | | | | | |")
        
        return "\n".join(lines)
    
    def _generate_operational_heatmap_placeholder(self) -> str:
        """Generate operational heatmap placeholder."""
        lines = [
            "## Operational Heatmap",
            "",
            "*Operational heatmap visualization will be added in a future release via new maps capability.*",
            "",
            "This section will provide:",
            "- Visual density/rate heatmaps across course segments",
            "- Time-based animation of crowd flows",
            "- Interactive bin-level detail views"
        ]
        return "\n".join(lines)
    
    def _generate_bin_level_detail(self, flagged_bins_df: pd.DataFrame) -> str:
        """Generate bin-level detail table for diagnostic visibility."""
        lines = [
            "## Bin-Level Detail",
            "",
            "Detailed bin-by-bin breakdown for segments with operational intelligence flags:",
            ""
        ]
        
        if len(flagged_bins_df) == 0:
            lines.append("*No flagged bins to display*")
            return "\n".join(lines)
        
        # Group by segment and sort by segment_id, then by t_start
        flagged_bins_sorted = flagged_bins_df.sort_values(['segment_id', 't_start'])
        
        current_segment = None
        for _, row in flagged_bins_sorted.iterrows():
            segment_id = row['segment_id']
            seg_label = row.get('seg_label', segment_id)
            
            # Start new segment section
            if current_segment != segment_id:
                if current_segment is not None:
                    lines.append("")  # Add spacing between segments
                
                current_segment = segment_id
                lines.extend([
                    f"### {seg_label} ({segment_id})",
                    "",
                    "| Start (km) | End (km) | Start (t) | End (t) | Density (p/mÂ²) | Rate (p/s) | LOS |",
                    "|------------|----------|-----------|---------|----------------|-------------|-----|"
                ])
            
            # Format time as HH:MM
            t_start = row['t_start']
            t_end = row['t_end']
            
            # Handle pandas Timestamp
            if hasattr(t_start, 'strftime'):
                start_time = t_start.strftime('%H:%M')
                end_time = t_end.strftime('%H:%M')
            # Handle string timestamps (ISO format)
            elif isinstance(t_start, str) and 'T' in t_start:
                start_time = t_start.split('T')[1][:5]  # HH:MM
                end_time = t_end.split('T')[1][:5] if isinstance(t_end, str) and 'T' in t_end else str(t_end)[:5]
            else:
                start_time = str(t_start)[:5]
                end_time = str(t_end)[:5]
            
            lines.append(
                f"| {row.get('start_km', 0):.1f} | {row.get('end_km', 0):.1f} | {start_time} | {end_time} | "
                f"{row['density']:.3f} | {row['rate']:.3f} | {row['los']} |"
            )
        
        return "\n".join(lines)
    
    def _generate_segment_details(self, segments_df: pd.DataFrame, segment_windows_df: pd.DataFrame, segment_summary_df: pd.DataFrame) -> str:
        """Generate segment details cards."""
        lines = [
            "## Segment Details"
        ]
        
        for _, seg_row in segments_df.iterrows():
            seg_id = seg_row['segment_id']
            seg_label = seg_row['seg_label']
            
            # Get summary data for this segment
            seg_summary = segment_summary_df[segment_summary_df['segment_id'] == seg_id]
            
            if len(seg_summary) > 0:
                summary = seg_summary.iloc[0]
                
                # Convert rate from p/m/min to p/s
                peak_rate_ps = summary['peak_rate_per_m_per_min'] / 60.0
                
                # Calculate Util% if we have rate thresholds for this schema
                segment_type = seg_row['segment_type']
                flow_ref_critical = None
                
                # Get flow_ref.critical from rulebook based on segment_type
                if segment_type == 'start_corral':
                    flow_ref_critical = 600  # runners/min/m
                elif segment_type == 'on_course_narrow':
                    flow_ref_critical = 400  # runners/min/m
                # on_course_open has no flow_ref
                
                util_pct = "N/A"
                if flow_ref_critical and summary['peak_rate_per_m_per_min'] > 0:
                    util_pct = f"{(summary['peak_rate_per_m_per_min'] / flow_ref_critical * 100):.0f}%"
                
                # Format worst bin details
                worst_km = f"{summary['worst_bin_start_km']:.1f}-{summary['worst_bin_end_km']:.1f}"
                worst_time = summary['worst_bin_t_start'].strftime('%H:%M') if hasattr(summary['worst_bin_t_start'], 'strftime') else str(summary['worst_bin_t_start']).split('T')[1][:5] if 'T' in str(summary['worst_bin_t_start']) else "N/A"
                
                # Build peaks line with Util%
                peaks_line = f"- **Peaks:** Density {summary['peak_density']:.4f} p/mÂ² (LOS {summary['peak_los']}), Rate {peak_rate_ps:.2f} p/s"
                if util_pct != "N/A":
                    peaks_line += f", Util {util_pct}"
                
                lines.extend([
                    "",
                    f"### {seg_label} ({seg_id})",
                    f"- **Schema:** {seg_row['segment_type']} Â· **Width:** {seg_row['width_m']} m Â· **Bins:** {summary['total_bins']}",
                    f"- **Active:** 07:00 â†’ 10:00",  # TODO: Calculate actual active times
                    peaks_line,
                    f"- **Worst Bin:** {worst_km} km at {worst_time} â€” {summary['worst_severity']} ({summary['worst_reason']})",
                    f"- **Mitigations:** {self._get_mitigations_for_segment(seg_id, summary['worst_reason'], seg_row['segment_type'])}"
                ])
            else:
                lines.extend([
                    "",
                    f"### {seg_label} ({seg_id})",
                    f"- **Schema:** {seg_row['segment_type']} Â· **Width:** {seg_row['width_m']} m",
                    "- **Status:** No operational flags"
                ])
        
        return "\n".join(lines)
    
    def _generate_mitigations(self, segment_summary_df: pd.DataFrame) -> str:
        """Generate consolidated mitigations list."""
        lines = [
            "## Mitigations"
        ]
        
        # Collect unique mitigation reasons
        flagged_segments = segment_summary_df[segment_summary_df['flagged_bins'] > 0]
        
        if len(flagged_segments) == 0:
            lines.append("No operational mitigations required.")
        else:
            lines.append("")
            mitigation_reasons = flagged_segments['worst_reason'].unique()
            
            for reason in mitigation_reasons:
                if reason == 'los_high':
                    lines.append("- **High Density (LOS â‰¥ C):** Monitor crowd density, consider flow management")
                elif reason == 'rate_high':
                    lines.append("- **High Throughput Rate:** Monitor flow rates, consider temporary holds")
                elif reason == 'both':
                    lines.append("- **High Density + High Rate:** Implement flow management and density controls")
        
        return "\n".join(lines)
    
    def _generate_appendix(self) -> str:
        """Generate appendix with definitions and thresholds."""
        lines = [
            "## Appendix",
            "",
            "### Definitions of Metrics",
            "- **Density (Ï):** Areal density in persons per square meter (p/mÂ²)",
            "- **Rate (q):** Throughput rate in persons per second (p/s)",
            "- **Rate per meter per minute:** (rate / width_m) Ã— 60 in persons/m/min",
            "- **Utilization (%):** Current flow rate / reference flow rate (critical)",
            "- **LOS (Level of Service):** Crowd comfort class (Aâ€“F)",
            "- **Bin:** Spaceâ€“time cell [segment_id, start_kmâ€“end_km, t_startâ€“t_end]",
            "",
            "### LOS Thresholds",
            "| LOS | Density Range (p/mÂ²) | Description |",
            "|-----|---------------------|-------------|",
            "| A | 0.00 - 0.36 | Free Flow |",
            "| B | 0.36 - 0.54 | Comfortable |",
            "| C | 0.54 - 0.72 | Moderate |",
            "| D | 0.72 - 1.08 | Dense |",
            "| E | 1.08 - 1.63 | Very Dense |",
            "| F | 1.63+ | Extremely Dense |",
            "",
            "### Trigger Logic & Severity",
            "- **los_high:** Density â‰¥ LOS C threshold",
            "- **rate_high:** Rate per m per min â‰¥ warning threshold",
            "- **both:** Both density and rate conditions met",
            "- **Severity:** critical > watch > none",
            "",
            "### Terminology Notes",
            "- \"Rate\" = persons/s (formerly \"Flow\")",
            "- Note: operational heatmap to be added in future release"
        ]
        
        return "\n".join(lines)
    
    def _get_mitigations_for_segment(self, segment_id: str, reason: str, segment_type: str = None) -> str:
        """Get mitigation text for a specific segment based on schema and reason."""
        if reason == 'none':
            return "No mitigations required"
        
        # Schema-specific base mitigations
        schema_mitigations = {
            'start_corral': "Expand chute width and manage wave timing",
            'on_course_narrow': "Deploy lateral barriers and regulate inflow",
            'on_course_open': "Monitor via visual flow sensors"
        }
        
        base_mitigation = schema_mitigations.get(segment_type, "Monitor flow management")
        
        # Add reason-specific actions
        if reason == 'los_high':
            return f"{base_mitigation}; monitor crowd density"
        elif reason == 'rate_high':
            return f"{base_mitigation}; consider temporary holds"
        elif reason == 'both':
            return f"{base_mitigation}; implement density controls and flow metering"
        else:
            return base_mitigation
