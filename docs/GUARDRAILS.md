
# Guardrails for AI-Assisted Development

**Version:** 1.1  (replaces /archive/guardrails.md)
**Last Updated:** 2025-10-27  
**Applies to:** Cursor, ChatGPT w/ Code Interpreter, GitHub Copilot (read-only), VSCode AI Agents (custom plugins)  

‚ö†Ô∏è **CRITICAL**: This document contains NON-NEGOTIABLE RULES for AI pair programming assistants. These rules were established through hard-learned lessons and MUST be followed without exception.

---

## üìã MANDATORY DOCUMENT REFERENCES

Before starting ANY work, you **MUST** reference these documents:

- `@GUARDRAILS.md` ‚Äì This document
- `@ARCHITECTURE.md` ‚Äì System design and core concepts
- `@VARIABLE_NAMING_REFERENCE.md` ‚Äì Authoritative variable names and field mappings
- `@OPERATIONS.md` ‚Äì Deployment, monitoring, and version control

---

## ‚úÖ MANDATORY RULE CONFIRMATION

Before any code or tests are written, confirm the following:

```
‚úÖ CONFIRMING CRITICAL RULES:
1. NO HARDCODED VALUES ‚Äì Use app/constants.py and config files only
2. PERMANENT CODE ONLY ‚Äì No temp scripts or isolated experiments
3. START TIME CONSTANTS ‚Äì Use constants.START_TIMES from app/constants.py
4. API TESTING ONLY ‚Äì Always test via app/main.py endpoints
5. MINIMAL CHANGES ‚Äì Make small, testable commits
6. NO ENDLESS LOOPS ‚Äì Stop after 3 failed analysis attempts and ask
7. STRICT TYPOS ‚Äì Match variable names exactly to references
8. NAMING CONVENTIONS ‚Äì Use field names from VARIABLE_NAMING_REFERENCE.md
9. TODO PERMISSION ‚Äì Ask before creating task lists or suggestions
10. GITHUB CONTEXT ‚Äì Read entire GitHub issue + all comments
11. CLARITY FIRST ‚Äì STOP and ask if any instruction is unclear
```

---

## üìÅ CRITICAL FILE REFERENCES

### ‚úÖ Always Use:
- `data/runners.csv`
- `data/segments.csv`
- `data/flow_expected_results.csv`
- `config/density_rulebook.yml`
- `config/reporting.yml`
- `app/constants.py`

### ‚ùå Never Use:
- `data/segments_old.csv`
- `data/your_pace_data.csv` (archived)

### Directory Structure
```
/app          ‚Äì Core application code
/config       ‚Äì YAML configuration
/data         ‚Äì Input CSVs only
/docs         ‚Äì Internal documentation
/frontend     ‚Äì Static files
/tests        ‚Äì Unit and integration tests
/reports      ‚Äì Auto-generated results
/cache        ‚Äì Git-ignored analysis cache
/archive      ‚Äì Historical snapshots
```

---

## üêç PYTHON ENVIRONMENT

### Activate Before All Python Work:
```bash
source test_env/bin/activate
```

### Deactivate for Git or CLI:
```bash
deactivate
```

Signs you forgot:
- `ModuleNotFoundError: No module named 'pandas'`
- Commands fail or misbehave

---

## üß™ TESTING GUARDRAILS

1. Activate venv
2. Use: `python e2e.py --local` for local testing
3. Do NOT test via curl or isolated modules
4. Validate output reports (CSV + MD)
5. Check constants are used, not hardcoded values

### For Unit Testing:
```bash
pytest tests/test_*.py
```

### For Cloud Run Validation:
```bash
TEST_CLOUD_RUN=true python e2e.py --cloud
```

---

## üîç GITHUB ISSUES

You MUST:
- Read full title, description, and **all** comments
- Use `gh issue view <number> --comments`
- Never summarize or skip subthreads
- GitHub is the source of truth - issues persist across Cursor sessions.
- Complete context is required - missing context leads to failed outcomes. Why critical:
  - Comments contain implementation strategies
  - ChatGPT answers provide specific technical details
  - User feedback and decisions documented
  - Incomplete reading causes rework

---

## üö´ PROHIBITED ACTIONS

- Developing in `main` - all dev/hotfix/bugfix must be done in branch
- Pushing to `main` directly
- Force pushing any branch
- Skipping E2E tests for logic-affecting changes
- Hardcoding thresholds, timing, or config logic
- Mixing time units (e.g., min/km with sec/km)
- Guessing API formats or variable names
- Creating unrequested todos
- Leaving ambiguity unresolved

---

## üîÅ MERGE AND RELEASE PROCESS

Follow this sequence:

1. ‚úÖ Confirm local branch health
2. ‚úÖ Run: `python e2e.py --local`
3. ‚úÖ Create PR with testing proof
4. ‚úÖ Wait for user to review and merge manually
5. ‚úÖ CI runs all Cloud Run tests: monitor workflow logs and Cloud Run logs during execution.
6. ‚úÖ Confirm CI workflow completed without errors. All 4 stages must pass (Build, E2E, Bin Datasets, Release). CI runs the full E2E tests on Cloud Run automatically
7. ‚úÖ Wait for the user to review the deployed code with manual testing via UI.
8. ‚úÖ Tag release and upload:
   - `Flow.csv`
   - `Density.md`

**When to Skip E2E Tests:**
- **Documentation-only changes** (README, GUARDRAILS, docs/)
- **Validation-only features** (frontend/validation/ - operates independently)
- **CI/CD configuration** (.github/workflows/ - validated by CI itself)
- **Non-code assets** (frontend/assets/, archive/)

**Monitoring Commands:**
```bash
# Check recent runs
gh run list --limit 5

# View run details
gh run view <run-id>

# Check logs for errors
gh run view <run-id> --log 2>&1 | grep -E "(ERROR|FAIL)"
```

---

## ‚òÅÔ∏è CLOUD RUN CONFIG (PROD)

- **URL**: https://run-density-...a.run.app
- **Timeout**: 600s
- **Resources**: 3GB / 2CPU (verify using Google CLI as resources might have changed)
- **Check**: `gcloud run services describe ...`

---

## üêõ COMMON FAILURE PATTERNS

### Zeroes in Reports?
1. **Check time unit consistency**
   - Start times: minutes ‚Üí seconds (`* 60.0`)
   - Pace: minutes/km ‚Üí seconds/km (`* 60.0`)
   - **Bug pattern**: `start_a = start_times.get(event_a, 0) * 60.0` but `start_b = start_times.get(event_b, 0)` ‚ùå

2. **Verify data filtering logic**
   - Check if filtered datasets are empty
   - Log boundary values
   - Validate intersection calculations

3. **Validate algorithm assumptions**
   - Reuse proven functions
   - Compare with working reference code
   - Test with known good data first

### CI Fails?
1. **Check /config directory** - Ensure Dockerfile copies it
2. **Verify resource constraints** - CI has required resources 3GB RAM / 2 CPU is recommended.
3. **Check schema changes** - Update validation scripts (e.g., `scripts/validation/verify_bins.py`)
4. **Review deployment logs** - `gh run view <run-id> --log`

---

## üìé RELEASE CHECKLIST

Every release needs:
- ‚úÖ `Flow.csv`, `Density.md` attached
- ‚úÖ Cloud Run validated
- ‚úÖ Version consistency with Git tag
- ‚úÖ Confirmation from CI logs

---

## üéØ SUCCESS CRITERIA

Work is complete ONLY when:
- ‚úÖ All code uses constants.py, no hardcoded values
- ‚úÖ All testing done through API endpoints
- ‚úÖ All reports generate correctly with proper formatting
- ‚úÖ All changes committed to feature branch
- ‚úÖ All validation tests pass
- ‚úÖ 9-step merge/test process completed
- ‚úÖ Production deployment verified (Cloud Run + local)
- ‚úÖ Release assets attached (if version bumped)

---

## üö® REMINDERS FOR AI AGENTS

- No session memory: re-check guardrails every time
- Never guess ‚Äî ask for clarity to user or ChatGPT as Senior Architect and QA Analyst
- Always test via documented endpoints
- Stop after ambiguity or repeated failures

**Common mistakes made by prior AI agents:**
Never do these things:
1. Hardcoding values instead of using constants
2. Creating temporary scripts instead of permanent code
3. Manually testing APIs instead of using e2e.py
4. Pushing to main without PR review
5. Skipping E2E tests after changes
6. Guessing production URLs instead of using documented values
7. Using inconsistent testing methodologies (local vs cloud)
8. Incomplete GitHub issue reading (missing comments)
9. Forgetting to activate the virtual environment before running e2e.py tests.
10. Mixing time units without conversion

**Remember**: These guardrails exist because they've been violated before, causing significant debugging overhead. Follow them strictly to maintain code quality and development velocity.