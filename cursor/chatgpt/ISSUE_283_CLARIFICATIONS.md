# Issue #283 - Implementation Clarifications

## Questions from Cursor

### 1. Relationship to Recent Work
In this session, we:
- Merged `main` into `rf-fe-002`, restoring `app/new_flagging.py`, `app/rulebook.py`, etc.
- Fixed `analytics/export_frontend_artifacts.py` to derive `flags.json` from `tooltips.json`
- Committed: `"fix(artifacts): derive flags.json from tooltips.json per ChatGPT recommendation"`

**Question:** Does `tooltips.json` already represent the canonical flagging logic? Is our recent fix a partial solution to Issue 283, or is it addressing a different problem?

### 2. Current Codebase Modules
We now have these flagging-related modules after the merge:
- `app/new_flagging.py` (from main)
- `app/rulebook.py` (from main) 
- `app/utilization.py` (from main)
- Logic in `app/density_report.py` / `app/new_density_report.py`

**Question:** Should the proposed `flagging.py` module be:
- A) A refactor/rename of `new_flagging.py`?
- B) A new wrapper that orchestrates existing modules?
- C) A complete replacement?

### 3. Source of Truth Clarification
The `bins.parquet` file (19,440 rows) already has columns:
- `flag_severity` (watch/none/etc.)
- `flag_reason` (utilization/etc.)
- `util_percentile`, `util_percent`, `los`, `los_class`

**Question:** Are these bin-level flags already correct/canonical? If so, is the issue just about:
- A) Aggregating bins → segment rollups consistently?
- B) Or are the bin-level flags themselves computed differently in different paths?

### 4. The 1875 vs 445 vs 12 Discrepancy
Looking at our data:
- **bins.parquet**: 19,440 bins total
- **Report says**: 1875 flagged bins across 17 segments
- **tooltips.json**: 12 segments with flags (we verified this)
- **Current flags.json**: 12 segments (matches tooltips after our fix)

**Question:** Which is the "ground truth"?
- A) Are there really 17 segments with flagged bins, and tooltips.json under-counts?
- B) Or are there 12 segments, and the report over-counts?
- C) Or are they measuring different things (e.g., different severity thresholds)?

### 5. Implementation Priority
Given that we just fixed `flags.json` to derive from `tooltips.json`:

**Question:** What's the recommended implementation sequence:
- A) Verify tooltips.json is canonical → propagate to report?
- B) Create new SSOT module → regenerate both tooltips.json and report?
- C) Something else?

---

## ChatGPT's Answers

### 1) Relationship to recent work (deriving flags.json from tooltips.json)

**Short answer:** Your change is a partial fix (helps the Dashboard=0 symptom), but it does **not** resolve Issue #283.

- Today, `tooltips.json` is **not canonical**. In the bundle we analyzed it contained only WATCH severities across 12 segments and was missing event coverage (e.g., 10K) and likely rate triggers.
- Therefore, deriving `flags.json` from `tooltips.json` just propagates the under-count into the UI artifacts (12/445), while the report (1875/17) remains correct.

**Conclusion:** Treat `tooltips.json` as a view, not the source of truth. After SSOT lands, regenerate `tooltips.json` from the SSOT bin flags so it becomes reliable.

---

### 2) Where should flagging.py live (new vs refactor)?

**Recommended:** **(A) Refactor/rename app/new_flagging.py into the canonical flagging.py API.**

- `new_flagging.py` already houses the modern rulebook-driven logic. Promote it to the public API (module name `flagging.py`) and:
  - expose two functions only: `compute_bin_flags(...)` and `summarize_flags(...)`;
  - make the report generator and artifact exporter import these (no re-implementation);
  - keep internal helpers (`rulebook.py`, `utilization.py`) as dependencies, but stop exporting ad-hoc flaggers elsewhere.

This avoids a third implementation and anchors the architecture.

---

### 3) Are bin-level flags in bins.parquet canonical?

**Yes**, with one guard: if the rulebook hash / version in the run metadata matches the report, then the flags in `bins.parquet` are the ground truth.

- You mentioned `bins.parquet` has `flag_severity`, `flag_reason`, `los`, etc. That's exactly what SSOT should consume.
- The discrepancy we saw comes from **aggregation/export** (tooltips/artifacts), not from the underlying bin computation.

**Action:** In SSOT, read flags directly from `bins.parquet` (or the in-memory bins prior to write), then `summarize_flags(...)` to produce segment roll-ups.

---

### 4) The 1875 vs 445 vs 12 discrepancy — which is truth?

**Ground truth:** **1875 flagged bins across 17 segments** (the report & the canonical bins).

`tooltips.json` and the pre-SSOT `flags.json` under-count because they were generated by older/partial logic (density-only, incomplete event coverage, alias names).

So pick **(A)** from your options.

---

### 5) Implementation priority / sequence

**Do (B):** Build SSOT and regenerate both report & UI artifacts from it.

**Order of work:**

1. **Refactor** `new_flagging.py` → `flagging.py` with the two exported functions.
2. **Report generator** reads `summarize_flags(...)` for Executive Summary & tables.
3. **Artifact exporter** writes:
   - `flags.json` ← `per_segment` from `summarize_flags`
   - `segment_metrics.json` ← same rollups (+ KPIs)
   - `tooltips.json` ← (if you keep it) serialize bin-level flags from SSOT (not re-computed)
4. **Parity test & CI gate** (see below).

---

## What Cursor needs to implement (explicit tasks)

### 1. Create canonical API

**File:** `app/flagging.py` (refactor from `new_flagging.py`)

**API:**
```python
def compute_bin_flags(bins, rulebook) -> list[BinFlag]: ...
def summarize_flags(bin_flags) -> dict: ...
```

- Severity order and LOS mapping live here; take both density & rate rules from `rulebook.py`.

### 2. Make both consumers depend on SSOT

- `app/new_density_report.py` (or `density_report.py`) imports `summarize_flags` (no local counting).
- `analytics/export_frontend_artifacts.py` imports `summarize_flags`; do **not** derive from `tooltips.json`. Instead:
  - Write `flags.json` and `segment_metrics.json` from `per_segment`.
  - Write `tooltips.json` from bin flags (optionally keep; but if you do, it must be serialized from SSOT bin flags).

### 3. Canonical field names

- **Use:** `segment_id`, `t_start`, `t_end`, `density`, `rate`, `los`, `severity`, `flagged_bins`.
- **Compatibility for one release:** include `seg_id` and `flagged_bin_count` aliases in JSON output and mark deprecated.

### 4. Include event coverage before flagging

- Ensure the same event routing the report uses (Full/Half/10K) is applied to bins prior to flag evaluation so segment inclusion is identical.

### 5. Remove duplicate flaggers

- Grep for any function re-computing flags in exporters/templates. Delete or forward them to `flagging.py`.

---

## Test Plan (ready to paste into PR)

### Unit: SSOT correctness

```python
def test_summarize_flags_counts(golden_bins, golden_rulebook):
    flags = compute_bin_flags(golden_bins, golden_rulebook)
    agg = summarize_flags(flags)
    assert agg["flagged_bin_total"] == 5
    assert set(agg["segments_with_flags"]) == {"X1", "Y1"}
    ps = {r["segment_id"]: r for r in agg["per_segment"]}
    assert ps["X1"]["flagged_bins"] == 3
    assert ps["Y1"]["flagged_bins"] == 2
```

### Integration: report vs artifacts parity

```python
def parse_exec_summary(md):
    import re
    fb = int(re.search(r"Flagged Bins:\s*(\d+)\s*/", md).group(1))
    swf = int(re.search(r"Segments with Flags:\s*(\d+)\s*/", md).group(1))
    return fb, swf

def get_fb(x): return x.get("flagged_bins") or x.get("flagged_bin_count") or 0

def test_parity(tmp_path):
    md = (tmp_path/"reports/Density.md").read_text()
    flags = json.loads((tmp_path/"artifacts/ui/flags.json").read_text())
    segm  = json.loads((tmp_path/"artifacts/ui/segment_metrics.json").read_text())

    md_bins, md_segs = parse_exec_summary(md)

    f_bins_sum = sum(get_fb(x) for x in flags)
    f_seg_cnt  = len({x.get("segment_id") or x.get("seg_id") for x in flags})
    assert md_bins == f_bins_sum
    assert md_segs == f_seg_cnt

    if isinstance(segm, list) and segm:
        sm_bins = sum(get_fb(x) for x in segm)
        sm_segs = len({x.get("segment_id") for x in segm})
        assert md_bins == sm_bins
        assert md_segs == sm_segs
```

### E2E: real dataset parity

- Run full pipeline (E2E).
- Assert report vs artifacts parity as above.
- Verify `flags.json` segments include 10K-covered segments when appropriate.

### FE sanity (optional)

- Selector sums `flagged_bins` (or `flagged_bin_count`) and counts unique `segment_id` → expect 1875 and 17 for this run.

---

## CI Gate (pre-publish)

Add a CI step that reads the just-built artifacts and fails on mismatch:
- `Density.md` Executive Summary vs `flags.json` & `segment_metrics.json`
- Check canonical field names present
- Ensure `flags.json` is non-empty

---

## One-time Migration Notes

- **Keep alias fields for 1 release:**
  - `flags.json`: also include `seg_id`, `flagged_bin_count`
  - FE can switch to `segment_id` + `flagged_bins` immediately; remove aliases in N+1.

---

## Quick Summary for Implementation

- **Is tooltips.json canonical?** No (today). It becomes canonical after it is generated from SSOT bin flags.
- **Where does flagging.py come from?** Refactor `new_flagging.py` → `flagging.py` and expose only the two API functions.
- **Are bin flags in bins.parquet authoritative?** Yes, if rulebook hash matches (they drive the report's 1875/17).
- **Which numbers are "truth"?** 1875/17 (report + bins). The 445/12 is a legacy under-count.
- **What to do first?** Build SSOT (refactor), then regenerate both report and artifacts from it.

---

**Source:** ChatGPT Technical Architecture Review (2025-10-20)

